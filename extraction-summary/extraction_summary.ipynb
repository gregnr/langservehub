{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f2c32e",
   "metadata": {},
   "source": [
    "## Document Loading\n",
    "\n",
    "Load a blog post on agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9fadce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "text = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086be03",
   "metadata": {},
   "source": [
    "## Connect to template\n",
    "\n",
    "`Context`\n",
    " \n",
    "* LangServe apps gives you access to templates.\n",
    "* Templates LLM pipeline (runnables or chains) end-points accessible via FastAPI.\n",
    "* The environment for these templates is managed by Poetry.\n",
    "\n",
    "`Create app`\n",
    "\n",
    "* Install LangServe and create an app.\n",
    "* This will create a new Poetry environment /\n",
    "```\n",
    "pip install < to add > \n",
    "langchain serve new my-app\n",
    "cd my-app\n",
    "```\n",
    "\n",
    "`Add templates`\n",
    "\n",
    "* When we add a template, we update the Poetry config file with the necessary dependencies.\n",
    "* It also automatically installed these template dependencies in your Poetry environment\n",
    "```\n",
    "langchain serve add extraction-summary\n",
    "```\n",
    "\n",
    "`Start FastAPI server`\n",
    "\n",
    "```\n",
    "langchain start\n",
    "```\n",
    "\n",
    "Note, we can now look at the endpoints:\n",
    "\n",
    "http://127.0.0.1:8000/docs#\n",
    "\n",
    "And look specifically at our loaded template:\n",
    "\n",
    "http://127.0.0.1:8000/docs#/default/invoke_extraction_summary_invoke_post\n",
    " \n",
    "We can also use remote runnable to call it.\n",
    "\n",
    "## Extraction or Tagging\n",
    "\n",
    "Depending on the prompt, we can use functions to do extraction or tagging.\n",
    "\n",
    "Here we will extraction fields using the schema specified in `chain.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed507784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve.client import RemoteRunnable\n",
    "extraction_model = RemoteRunnable('http://localhost:8000/extraction-summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68046695",
   "metadata": {},
   "source": [
    "The function call will summarize, provide keywords, and provide language for the input passage, as specified in extraction schema in `chain.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dace748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'Overview', 'arguments': '{\\n  \"summary\": \"This blog post discusses the concept of building autonomous agents powered by LLM (large language model). It covers the agent system overview, including planning, memory, and tool use. It also provides case studies and examples of LLM-powered agents. The challenges and references are also mentioned.\",\\n  \"language\": \"English\",\\n  \"keywords\": \"LLM, autonomous agents, planning, memory, tool use, case studies\"\\n}'}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_model.invoke(text[0].page_content[0:1500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langserve",
   "language": "python",
   "name": "langserve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
