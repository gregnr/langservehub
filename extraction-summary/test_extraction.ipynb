{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f2c32e",
   "metadata": {},
   "source": [
    "## Document Loading\n",
    "\n",
    "Load a blog post on agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9fadce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "text = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086be03",
   "metadata": {},
   "source": [
    "## Connect to template\n",
    "\n",
    "`Context`\n",
    " \n",
    "* LangServe apps gives you access to templates.\n",
    "* Templates LLM pipeline (runnables or chains) end-points accessible via FastAPI.\n",
    "* The environment for these templates is managed by Poetry.\n",
    "\n",
    "`Create app`\n",
    "\n",
    "* Install LangServe and create an app.\n",
    "* This will create a new Poetry environment /\n",
    "```\n",
    "pip install < to add > \n",
    "langchain serve new my-app\n",
    "cd my-app\n",
    "```\n",
    "\n",
    "`Add templates`\n",
    "\n",
    "* When we add a template, we update the Poetry config file with the necessary dependencies.\n",
    "* It also automatically installed these template dependencies in your Poetry environment\n",
    "```\n",
    "langchain serve add extraction-summary\n",
    "```\n",
    "\n",
    "`Start FastAPI server`\n",
    "\n",
    "```\n",
    "langchain start\n",
    "```\n",
    "\n",
    "Note, we can now look at the endpoints:\n",
    "\n",
    "http://127.0.0.1:8000/docs#\n",
    "\n",
    "And look specifically at our loaded template:\n",
    "\n",
    "http://127.0.0.1:8000/docs#/default/invoke_extraction_summary_invoke_post\n",
    " \n",
    "We can also use remote runnable to call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed507784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve.client import RemoteRunnable\n",
    "extraction_model = RemoteRunnable('http://localhost:8000/extraction-summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68046695",
   "metadata": {},
   "source": [
    "The function call will summarize, provide keywords, and provide language for the input passage, as specified in extraction schema in `chain.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dace748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'Overview', 'arguments': '{\\n  \"summary\": \"This article discusses the concept of building autonomous agents powered by LLM (large language model). It explores the key components of such agents, including planning, memory, and tool use. It also presents case studies and challenges associated with building LLM-powered agents.\",\\n  \"language\": \"English\",\\n  \"keywords\": \"LLM, autonomous agents, planning, memory, tool use, case studies, challenges\"\\n}'}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_model.invoke(text[0].page_content[0:1500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langserve",
   "language": "python",
   "name": "langserve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
