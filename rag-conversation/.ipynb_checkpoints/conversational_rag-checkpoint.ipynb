{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "424a9d8d",
   "metadata": {},
   "source": [
    "## Connect to template\n",
    "\n",
    "`Context`\n",
    " \n",
    "* LangServe apps gives you access to templates.\n",
    "* Templates LLM pipeline (runnables or chains) end-points accessible via FastAPI.\n",
    "* The environment for these templates is managed by Poetry.\n",
    "\n",
    "`Create app`\n",
    "\n",
    "* Install LangServe and create an app.\n",
    "* This will create a new Poetry environment /\n",
    "```\n",
    "pip install < to add > \n",
    "langchain serve new my-app\n",
    "cd my-app\n",
    "```\n",
    "\n",
    "`Add templates`\n",
    "\n",
    "* When we add a template, we update the Poetry config file with the necessary dependencies.\n",
    "* It also automatically installed these template dependencies in your Poetry environment\n",
    "```\n",
    "langchain serve add conversational-rag\n",
    "```\n",
    "\n",
    "`Start FastAPI server`\n",
    "\n",
    "```\n",
    "langchain start\n",
    "```\n",
    "\n",
    "Note, we can now look at the endpoints:\n",
    "\n",
    "http://127.0.0.1:8000/docs#\n",
    "\n",
    "And look specifically at our loaded template:\n",
    "\n",
    "http://127.0.0.1:8000/docs#/default/invoke_conversationa_rag_invoke_post\n",
    " \n",
    "We can also use remote runnable to call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f521923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve.client import RemoteRunnable\n",
    "rag_app = RemoteRunnable('http://localhost:8000/conversational-rag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce206c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Based on the provided context, the agent memory in LLM-powered autonomous agent systems is described as a \"long-term memory module\" that records a comprehensive list of agents\\' experiences in natural language. Each element is an observation, an event directly provided by the agent, and inter-agent communication can trigger new natural language statements.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_app.invoke({\n",
    "    \"question\": \"How does agent memory work?\",\n",
    "    \"chat_history\": [],\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
