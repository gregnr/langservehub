{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8895c4b",
   "metadata": {},
   "source": [
    "## Document Loading\n",
    "\n",
    "Load a blog post on agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d0bfdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "text = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e596fe",
   "metadata": {},
   "source": [
    "## Local LLM\n",
    "\n",
    "As shown in the README, setup your local LLM via `ollama.ai`.\n",
    "\n",
    "## Connect to template\n",
    "\n",
    "`Context`\n",
    " \n",
    "* LangServe apps gives you access to templates.\n",
    "* Templates LLM pipeline (runnables or chains) end-points accessible via FastAPI.\n",
    "* The environment for these templates is managed by Poetry.\n",
    "\n",
    "`Create app`\n",
    "\n",
    "* Install LangServe and create an app.\n",
    "* This will create a new Poetry environment /\n",
    "```\n",
    "pip install < to add > \n",
    "langchain serve new my-app\n",
    "cd my-app\n",
    "```\n",
    "\n",
    "`Add templates`\n",
    "\n",
    "* When we add a template, we update the Poetry config file with the necessary dependencies.\n",
    "* It also automatically installed these template dependencies in your Poetry environment\n",
    "```\n",
    "langchain serve add rag-chroma-private\n",
    "```\n",
    "\n",
    "`Start FastAPI server`\n",
    "\n",
    "```\n",
    "langchain start\n",
    "```\n",
    "\n",
    "Note, we can now look at the endpoints:\n",
    "\n",
    "http://127.0.0.1:8000/docs#\n",
    "\n",
    "And look specifically at our loaded template:\n",
    "\n",
    "http://127.0.0.1:8000/docs#/default/invoke_rag_chroma_private_invoke_post\n",
    " \n",
    "We can also use remote runnable to call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8155adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve.client import RemoteRunnable\n",
    "rag_app = RemoteRunnable('http://localhost:8000/rag-chroma-private')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c956025e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Based on the provided context, the agent memory in LLM-powered autonomous agent systems is described as a \"long-term memory module\" that records a comprehensive list of agents\\' experiences in natural language. Each element is an observation, an event directly provided by the agent, and inter-agent communication can trigger new natural language statements.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_app.invoke(\"How does agent memory work?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
